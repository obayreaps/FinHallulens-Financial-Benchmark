{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create a pip3 install command that will install all packages required to run this notebook\n",
    "!pip3 install -U --force-reinstall --no-cache-dir \\\n",
    "    torch == 2.6.0 \\\n",
    "    transformers \\\n",
    "    datasets \\\n",
    "    vllm \\\n",
    "    pandas \\\n",
    "    tqdm \\\n",
    "    scikit-learn"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Test performance"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "TMP_DIR = \"/tmp/\"\n",
    "\n",
    "data_names = ['10k_seed', '8k_seed', '497k_seed', 'def14a_seed',\n",
    "              '10k_20000tokens_beginning', '10k_20000tokens_end', '10k_20000tokens_middle',\n",
    "              '10k_2000tokens_beginning', '10k_2000tokens_end', '10k_2000tokens_middle',\n",
    "              '50000tokens_beginning', '50000tokens_end', '50000tokens_middle',\n",
    "              '10k_5000tokens_beginning', '10k_5000tokens_end', '10k_5000tokens_middle',\n",
    "              '10k_30000tokens_end','10k_30000tokens_beginning','10k_30000tokens_middle',\n",
    "              '10k_10000tokens_beginning', '10k_10000tokens_end', '10k_10000tokens_middle'\n",
    "              'def14A_10000tokens_beginning', 'def14A_10000tokens_end', 'def14A_10000tokens_middle'\n",
    "              'def14A_2000tokens_middle','def14A_2000tokens_beginning','def14A_2000tokens_end',\n",
    "              'def14A_5000tokens_middle','def14A_5000tokens_beginning','def14A_5000tokens_end',\n",
    "              'def14A_20000tokens_middle','def14A_20000tokens_end','def14A_20000tokens_beginning',\n",
    "              'def14A_30000tokens_end','def14A_30000tokens_beginning','def14A_30000tokens_middle',\n",
    "              'def14A_10000tokens_beginning', 'def14A_10000tokens_end', 'def14A_10000tokens_middle'\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "detector_model_list = ['deepseek-ai/DeepSeek-R1-Distill-Qwen-7B', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-14B', 'deepseek-ai/DeepSeek-R1-Distill-Qwen-32B',\n",
    "                       \"Qwen/Qwen2.5-14B-Instruct\", \"Qwen/Qwen2.5-32B-Instruct\", \"Qwen/Qwen2.5-7B-Instruct\", \"meta-llama/Llama-3.3-70B-Instruct\",\n",
    "                       \"deepseek-ai/DeepSeek-R1-Distill-Llama-70B\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "today = datetime.date.today()\n",
    "time_now = datetime.datetime.now()\n",
    "\n",
    "folder_name = str(today)\n",
    "path = os.path.join(TMP_DIR, \"results\", folder_name)\n",
    "\n",
    "try:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    print(f\"Directory '{folder_name}' created or already exists.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error creating directory: {e}\")\n",
    "\n",
    "model_names_str = '_'.join(detector_model_list)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Put your prompt for hallucination detection in the function below (extend HALLUCINATION_PROMPT string).\n",
    "\n",
    "We follow these guidelines when creating the hallucination detection prompt:\n",
    "\t1. Detect any deviation in the answer from the context.\n",
    "\t2. Detect implied or implicit information in answer that is not present in the context.\n",
    "\t3. Detect misalignment of timing in the answer that might be different from the context.\n",
    "\t4. Detect any important details in the context that is missed in the answer.\n",
    "\t5. Ask the model to provide reasoning (i.e., chain of thought).\n",
    "\t6. Ask for pass/fail decision, depending on whether the models finds the answer faithful to the context or not.\n",
    "\"\"\"\n",
    "def create_detection_prompt(query, chunk, answer):\n",
    "        HALLUCINATION_PROMPT = f\"\"\"\n",
    "        **Input Format:**\n",
    "        --\n",
    "        QUESTION:\n",
    "        {query}\n",
    "        CONTEXT:\n",
    "        {chunk}\n",
    "        ANSWER:\n",
    "        {answer}\n",
    "        --\n",
    "\n",
    "        **Example output:**\n",
    "        {{\"REASONING\": [...], \"SCORE\": \"PASS\" or \"FAIL\"}}\n",
    "        \"\"\"\n",
    "        return HALLUCINATION_PROMPT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class VllmModel:\n",
    "    \"\"\"\n",
    "    A class for interacting with the language model using vLLM for fast inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str = \"Qwen/Qwen2.5-7B-Instruct\", tensor_parallel_size: int = 2,\n",
    "                max_token_length=32000):\n",
    "        \"\"\"Initializes the model with the specified model path and GPU configuration.\n",
    "\n",
    "        Args:\n",
    "            model_path (str, optional): The path or identifier for the Qwen model.\n",
    "                Defaults to \"Qwen/Qwen2.5-7B-Instruct\".\n",
    "            tensor_parallel_size (int, optional): Number of GPUs to use for multi-GPU model parallelism.\n",
    "                Defaults to 2.\n",
    "        \"\"\"\n",
    "        self.model_path = model_path\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "        self.sampling_params = SamplingParams(\n",
    "            temperature=0.7,\n",
    "            top_p=0.8,\n",
    "            repetition_penalty=1.05,\n",
    "            max_tokens=8000\n",
    "        )\n",
    "        self.llm = LLM(model=model_path, tensor_parallel_size=tensor_parallel_size)\n",
    "        self.max_token_length = max_token_length\n",
    "\n",
    "    def generate_prompts(self, prompts: List[str]):\n",
    "        \"\"\"Prepares prompts for generation and yields the model outputs.\n",
    "\n",
    "        Args:\n",
    "            prompts (List[str]): A list of prompt strings.\n",
    "\n",
    "        Returns:\n",
    "            vllm.outputs.RequestOutput: The generated output for each prompt.\n",
    "        \"\"\"\n",
    "        model_input = []\n",
    "        for prompt in tqdm(prompts, desc=\"Generating outputs\"):\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            chat_text = self.tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            encoded_input = self.tokenizer.encode(chat_text, truncation=False)\n",
    "\n",
    "            if len(encoded_input) > self.max_token_length:\n",
    "                truncated_input = encoded_input[: self.max_token_length]\n",
    "                chat_text = self.tokenizer.decode(truncated_input, skip_special_tokens=True)\n",
    "            model_input.append(chat_text)\n",
    "\n",
    "        outputs = self.llm.generate(model_input, self.sampling_params)\n",
    "        return outputs\n",
    "\n",
    "    def generate_responses(self, prompts: List[str], return_answer_only: bool = True):\n",
    "        \"\"\"Generates responses for the provided prompts.\n",
    "        Optionally returns only the answer text from a full output object.\n",
    "\n",
    "        Args:\n",
    "            prompts (List[str]): A list of prompt strings.\n",
    "            return_answer_only (bool, optional): Whether to return only the answer text.\n",
    "                Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            List[str] or List[vllm.outputs.RequestOutput]: A list of generated answer texts or full output objects.\n",
    "        \"\"\"\n",
    "        outputs = self.generate_prompts(prompts)\n",
    "        if return_answer_only:\n",
    "            outputs = [output.outputs[0].text for output in outputs]\n",
    "        return outputs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "re_map = {\n",
    "    \"pass\": \"not hallucination\",\n",
    "    \"fail\": \"hallucination\"\n",
    "}\n",
    "\n",
    "def get_label(response):\n",
    "    \"\"\"\n",
    "    Parses a string containing JSON-like model response to extract the 'SCORE' and determine if it indicates a hallucination.\n",
    "\n",
    "    Args:\n",
    "        response: A string containing JSON-like data.\n",
    "\n",
    "    Returns:\n",
    "        \"not hallucination\" if the SCORE is \"PASS\", \"hallucination\" if the SCORE is \"FAIL\".\n",
    "        Returns None if the SCORE cannot be determined.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Attempt to load the response as JSON\n",
    "        j_res = json.loads(response).lower()\n",
    "        score = j_res.get(\"score\").strip()\n",
    "        if score in re_map:\n",
    "            return re_map[score]\n",
    "        else:\n",
    "            # throw exception\n",
    "            raise ValueError(f\"Unexpected score value: {score}\")\n",
    "\n",
    "    except:\n",
    "        # If JSON parsing fails, use regular expressions to find the SCORE\n",
    "        try:\n",
    "            match = re.search(r'\"SCORE\"\\s*:\\s*\"(\\w+)\"', response, re.IGNORECASE)\n",
    "            if match:\n",
    "                score = match.group(1).lower().strip()\n",
    "                if score in re_map:\n",
    "                    return re_map[score]\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected score value: {score}\")\n",
    "            else:\n",
    "                if '\"FAIL\"' in response:\n",
    "                    return 'hallucination'\n",
    "                else:\n",
    "                    return 'not hallucination'\n",
    "        except:\n",
    "            return 'not hallucination'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## RUN EXPERIMENT\n",
    "for detector_model_name in detector_model_list:\n",
    "    model = VllmModel(detector_model_name, tensor_parallel_size=4, max_token_length=32000)\n",
    "    for data_name in data_names:\n",
    "        print(data_name)\n",
    "\n",
    "        print(f'DATA: Phantom {data_name}')\n",
    "        print(f'MODELS: {detector_model_list}')\n",
    "\n",
    "        ## LOAD DATA\n",
    "        df = load_dataset(\"seyled/Phantom_Hallucination_Detection\", data_files=f\"PhantomDataset/Phantom_{data_name}.csv\")\n",
    "        df_final = df[\"train\"].to_pandas()\n",
    "\n",
    "        output_file_name = f'{path}/outputs_{detector_model_name.replace(\"/\",\"__\")}_{data_name}_{time_now.strftime(\"%Y-%m-%d_%H:%M\")}.csv'\n",
    "        result_file_name = f'{path}/results_{detector_model_name.replace(\"/\",\"__\")}_{data_name}_{time_now.strftime(\"%Y-%m-%d_%H:%M\")}.txt'\n",
    "        print(f'''Results stored in '{output_file_name}', '{result_file_name}' ''')\n",
    "\n",
    "        print('\\nStart testing ', detector_model_name)\n",
    "        start_time = time.time()\n",
    "\n",
    "        df_final[f'{detector_model_name}_output'] = None\n",
    "\n",
    "        prompt_list = []\n",
    "        for i in range(df_final.shape[0]):\n",
    "            query = df_final.at[i, 'query']\n",
    "            chunk = df_final.at[i, 'context']\n",
    "            answer = df_final.at[i, 'answer']\n",
    "\n",
    "            HALLUCINATION_PROMPT = create_detection_prompt(query, chunk, answer)\n",
    "            prompt_list.append(HALLUCINATION_PROMPT)\n",
    "\n",
    "        model_output_list = model.generate_responses(prompt_list)\n",
    "\n",
    "        df_final[f'{detector_model_name}_output'] = model_output_list\n",
    "        print('\\nFinished testing ', detector_model_name)\n",
    "        print(\"Process time: \", round(time.time() - start_time, 2), \" seconds.\")\n",
    "\n",
    "        ## SAVE FILE\n",
    "        df_final.to_csv(output_file_name)\n",
    "\n",
    "    ### PARSE OUTPUT\n",
    "        print('\\nStart parsing ', detector_model_name, ' results.')\n",
    "\n",
    "        df_final[f'{detector_model_name}_label'] = None\n",
    "\n",
    "        for i in range(df_final.shape[0]):\n",
    "            df_final.at[i, f'{detector_model_name}_label'] = get_label(df_final.at[i, f'{detector_model_name}_output'])\n",
    "        print('\\nFinished parsing ', detector_model_name)\n",
    "\n",
    "    ### OUTPUT\n",
    "        with open(result_file_name, 'w') as f:\n",
    "            f.write(f'Phantom {data_name} results:')\n",
    "            print(f'Phantom {data_name} results:')\n",
    "            results = classification_report(df_final['ground_truth_label'], df_final[f'{detector_model_name}_label'],\n",
    "                                            digits=3)\n",
    "            f.write(f'\\n\\n{detector_model_name} results.')\n",
    "            f.write(results)\n",
    "            print(f'\\n\\n{detector_model_name} results.')\n",
    "            print(results)\n",
    "\n",
    "    del model\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
