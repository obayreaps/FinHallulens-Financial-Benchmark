{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83aa077-2fe2-4f10-9b61-738217cf4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f496c6-0449-4d2b-b257-3f2ccb7cb107",
   "metadata": {},
   "source": [
    "### Load existing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e12b01-5124-48fe-b9e3-8efeb80da9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk = pd.read_csv('df_10k_llama3_3_v1.csv', index_col = 0) # Load seed dataset created in Phantom_generate_seed_hallucination_data.ipynb\n",
    "df_data_chunk['chunk_token_count'] = df_data_chunk['chunk_token_count'].astype(int)\n",
    "df_data_chunk['document_token_count'] = df_data_chunk['document_token_count'].astype(int)\n",
    "df_data_chunk['chunk_starting_sentence_idx'] = df_data_chunk['chunk_starting_sentence_idx'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7776398-3c7d-4e6c-84e5-d0106ace7138",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fc1edf-5ef8-43a4-a3e7-b7ecc6a2ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd11b6e-8022-4b30-889f-1e433344649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "expand_token_count = 10000 ## Context Limit update for 2k,5k,10k,20k,30k\n",
    "token_limit = 10500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daedd1c9-e57f-4494-9b23-8f1ef4483d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk = df_data_chunk[df_data_chunk[\"document_token_count\"] >= expand_token_count].copy()\n",
    "df_data_chunk = df_data_chunk.reset_index(drop = True)\n",
    "df_data_chunk.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29027aa7-9fdc-4f6e-80fc-672070149b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510a2000-0be4-4466-ae0a-8b29d826ae57",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Long context Expansion - END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f8ec0d-84a0-4a20-b6e3-67c113173cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data_end = df_data_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fd2afe-e16e-4930-95a3-9436f2935cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data_end.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436f8da7-dbee-415d-8350-a0a795e1a5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def expand_chunk_end(nlp, chunk_context, chunk_token_count, doc_text, chunk_starting_sentence_idx, expand_token_count=2000, token_limit=2200):\n",
    "  \n",
    "    doc = nlp(doc_text)\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = list(doc.sents)\n",
    "    n_sent = len(sentences)\n",
    "    \n",
    "    chunk = [chunk_context]\n",
    "\n",
    "    current_sent_idx = chunk_starting_sentence_idx - 1\n",
    "    while True:\n",
    "        current_sent_idx %= n_sent\n",
    "        sent = sentences[current_sent_idx]\n",
    "\n",
    "        new_token_count = chunk_token_count + len(sent)\n",
    "        if new_token_count >= expand_token_count and new_token_count <= token_limit:\n",
    "            chunk_token_count = new_token_count\n",
    "            chunk = [sent.text] + chunk\n",
    "            chunk_starting_sentence_idx = current_sent_idx\n",
    "            break\n",
    "        elif new_token_count >= expand_token_count and new_token_count > token_limit:   \n",
    "            partial_first_sentence = \" \".join([token.text for token in sent[(len(sent) - (token_limit - chunk_token_count)):]])\n",
    "            chunk = [partial_first_sentence] + chunk\n",
    "            chunk_token_count = token_limit\n",
    "            break\n",
    "        chunk_token_count = new_token_count\n",
    "        chunk = [sent.text] + chunk\n",
    "        chunk_starting_sentence_idx = current_sent_idx\n",
    "        \n",
    "        current_sent_idx -= 1\n",
    "    \n",
    "    # Join the sentences to form the chunk\n",
    "    chunk_text = \" \".join(chunk)\n",
    "       \n",
    "    return {\n",
    "        \"chunk\": chunk_text,\n",
    "        \"chunk_token_count\": chunk_token_count,\n",
    "        \"chunk_starting_sentence_idx\": chunk_starting_sentence_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72eb1fa4-d621-4b21-8e48-2d265bc9f98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## END\n",
    "\n",
    "start_time_total = time.time()\n",
    "start_time = time.time()\n",
    "for i in range(df_generated_data_end.shape[0]):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "        print(\"Process time: \", round(time.time()-start_time, 2), \" seconds.\")\n",
    "        start_time = time.time()\n",
    "\n",
    "    chunk_context = df_data_chunk.at[i, 'context']\n",
    "    chunk_token_count = df_generated_data_end.at[i, 'chunk_token_count']\n",
    "    doc_text = df_generated_data_end.at[i, 'filing_text']\n",
    "    chunk_starting_sentence_idx = df_generated_data_end.at[i, 'chunk_starting_sentence_idx'] \n",
    "\n",
    "    chunk_result = expand_chunk_end(nlp, chunk_context, chunk_token_count, doc_text, chunk_starting_sentence_idx,\n",
    "                                expand_token_count, token_limit)\n",
    "\n",
    "    df_generated_data_end.at[i, 'context'] = chunk_result[\"chunk\"]\n",
    "    df_generated_data_end.at[i, 'chunk_token_count'] = chunk_result[\"chunk_token_count\"]\n",
    "    df_generated_data_end.at[i, 'chunk_starting_sentence_idx'] = chunk_result[\"chunk_starting_sentence_idx\"]\n",
    "print(\"Total process time: \", round(time.time()-start_time_total, 2), \" seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44956d88-0a7d-4c5c-ad8a-4040d67f3e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_generated_data_end['chunk_token_count'], bins=30)\n",
    "plt.xlabel('Token count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of chunk length (token count)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c5cf84-ed41-4e41-bf52-3edcb949270b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data_end.to_csv('Phantom_10k_10000tokens_end_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4a185-1002-41f2-a86e-e7814dc3dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_end = pd.DataFrame(columns = ['query', 'context', 'answer', 'ground_truth_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dd2bd-c511-4e24-8c8d-897b67ed0f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_generated_data_end.shape[0]):\n",
    "    query = df_generated_data_end.at[i, 'query']\n",
    "    context = df_generated_data_end.at[i, 'context']\n",
    "    row_data1 = [query, context, df_generated_data_end.at[i, 'gold_answer'], 'not hallucination']\n",
    "    row_data2 = [query, context, df_generated_data_end.at[i, 'hallucination_answer'], 'hallucination']\n",
    "    df_final_end.loc[len(df_final_end)] = row_data1\n",
    "    df_final_end.loc[len(df_final_end)] = row_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40906a-dd36-475d-9a4b-1dc183537b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_end.to_csv('Phantom_10k_10000tokens_end.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38466c2-4be2-4c29-a8ea-f233bc0c74c3",
   "metadata": {},
   "source": [
    "## Long context - BEGINNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ee4bd9-4791-477f-8ffb-48cbf5cef7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data_begin = df_data_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1510ff7-2255-4d9e-9087-715284ccedf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def expand_chunk_begin(nlp, doc_text, chunk_starting_sentence_idx, expand_token_count=2000, token_limit=2200):\n",
    "  \n",
    "    doc = nlp(doc_text)\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = list(doc.sents)\n",
    "    n_sent = len(sentences)\n",
    "    \n",
    "    chunk = []\n",
    "    chunk_token_count = 0\n",
    "\n",
    "    current_sent_idx = chunk_starting_sentence_idx\n",
    "    while True:\n",
    "        current_sent_idx %= n_sent\n",
    "        sent = sentences[current_sent_idx]\n",
    "        \n",
    "        chunk.append(sent.text)\n",
    "        new_token_count = chunk_token_count + len(sent)\n",
    "        if new_token_count >= expand_token_count and new_token_count <= token_limit:\n",
    "            chunk_token_count = new_token_count\n",
    "            break\n",
    "        elif new_token_count >= expand_token_count and new_token_count > token_limit:   \n",
    "            partial_last_sentence = \" \".join([token.text for token in sent[:(token_limit - chunk_token_count)]])\n",
    "            chunk[-1] = partial_last_sentence\n",
    "            chunk_token_count = token_limit\n",
    "            break\n",
    "        chunk_token_count = new_token_count \n",
    "        current_sent_idx += 1\n",
    "    \n",
    "    # Join the sentences to form the chunk\n",
    "    chunk_text = \" \".join(chunk)\n",
    "       \n",
    "    return {\n",
    "        \"chunk\": chunk_text,\n",
    "        \"chunk_token_count\": chunk_token_count,\n",
    "        \"chunk_starting_sentence_idx\": chunk_starting_sentence_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60bbb0f-fa76-4848-95c9-dda2344f74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(df_generated_data_begin.shape[0]):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "        print(\"Process time: \", round(time.time()-start_time, 2), \" seconds.\")\n",
    "        start_time = time.time()\n",
    "    \n",
    "    doc_text = df_generated_data_begin.at[i, 'filing_text']\n",
    "    chunk_starting_sentence_idx = df_generated_data_begin.at[i, 'chunk_starting_sentence_idx'] \n",
    "    \n",
    "    chunk_result = expand_chunk_begin(nlp, doc_text, chunk_starting_sentence_idx, expand_token_count, token_limit)\n",
    "\n",
    "    df_generated_data_begin.at[i, 'context'] = chunk_result[\"chunk\"]\n",
    "    df_generated_data_begin.at[i, 'chunk_token_count'] = chunk_result[\"chunk_token_count\"]\n",
    "    df_generated_data_begin.at[i, 'chunk_starting_sentence_idx'] = chunk_result[\"chunk_starting_sentence_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930296a8-fb13-436d-b83f-c83ec33e9b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_generated_data_begin['chunk_token_count'], bins=30)\n",
    "plt.xlabel('Token count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of chunk length (token count)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f495add-b19c-4079-87dc-8eaa342a0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data_begin.to_csv('Phantom_10k_10000tokens_beginning_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f446593-47fd-4d56-99f4-9144924ca919",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_begin = pd.DataFrame(columns = ['query', 'context', 'answer', 'ground_truth_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18606b4a-e0a9-4d48-b535-15b90b588c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_generated_data_begin.shape[0]):\n",
    "    query = df_generated_data_begin.at[i, 'query']\n",
    "    context = df_generated_data_begin.at[i, 'context']\n",
    "    row_data1 = [query, context, df_generated_data_begin.at[i, 'gold_answer'], 'not hallucination']\n",
    "    row_data2 = [query, context, df_generated_data_begin.at[i, 'hallucination_answer'], 'hallucination']\n",
    "    df_final_begin.loc[len(df_final_begin)] = row_data1\n",
    "    df_final_begin.loc[len(df_final_begin)] = row_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7c49e-02a9-4eb4-b6c7-2c86b8ca9bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_begin.to_csv('Phantom_10k_10000tokens_beginning.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e92cd-6ffb-43b6-a337-8e7e6c5cc550",
   "metadata": {},
   "source": [
    "## Long context - MIDDLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92879c5-67db-471d-9c8e-4bd69023d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data_middle = df_data_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec8845-b48b-4315-9114-42100c84f777",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def expand_chunk_middle_1(nlp, doc_text, chunk_starting_sentence_idx, expand_token_count=2000, token_limit=2200):\n",
    "  \n",
    "    doc = nlp(doc_text)\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = list(doc.sents)\n",
    "    n_sent = len(sentences)\n",
    "    \n",
    "    chunk = []\n",
    "    chunk_token_count = 0\n",
    "\n",
    "    current_sent_idx = chunk_starting_sentence_idx\n",
    "    while True:\n",
    "        current_sent_idx %= n_sent\n",
    "        sent = sentences[current_sent_idx]\n",
    "        \n",
    "        chunk.append(sent.text)\n",
    "        new_token_count = chunk_token_count + len(sent)\n",
    "        if new_token_count >= expand_token_count and new_token_count <= token_limit:\n",
    "            chunk_token_count = new_token_count\n",
    "            break\n",
    "        elif new_token_count >= expand_token_count and new_token_count > token_limit:   \n",
    "            partial_last_sentence = \" \".join([token.text for token in sent[:(token_limit - chunk_token_count)]])\n",
    "            chunk[-1] = partial_last_sentence\n",
    "            chunk_token_count = token_limit\n",
    "            break\n",
    "        chunk_token_count = new_token_count \n",
    "        current_sent_idx += 1\n",
    "    \n",
    "    # Join the sentences to form the chunk\n",
    "    chunk_text = \" \".join(chunk)\n",
    "       \n",
    "    return {\n",
    "        \"chunk\": chunk_text,\n",
    "        \"chunk_token_count\": chunk_token_count,\n",
    "        \"chunk_starting_sentence_idx\": chunk_starting_sentence_idx,\n",
    "        \"sentences\": sentences\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def expand_chunk_middle_2(sentences, chunk_context, chunk_token_count, doc_text, chunk_starting_sentence_idx, \n",
    "                          expand_token_count=2000, token_limit=2200):\n",
    "    n_sent = len(sentences)\n",
    "    \n",
    "    chunk = [chunk_context]\n",
    "\n",
    "    current_sent_idx = chunk_starting_sentence_idx - 1\n",
    "    while True:\n",
    "        current_sent_idx %= n_sent\n",
    "        sent = sentences[current_sent_idx]\n",
    "\n",
    "        new_token_count = chunk_token_count + len(sent)\n",
    "        if new_token_count >= expand_token_count and new_token_count <= token_limit:\n",
    "            chunk_token_count = new_token_count\n",
    "            chunk = [sent.text] + chunk\n",
    "            chunk_starting_sentence_idx = current_sent_idx\n",
    "            break\n",
    "        elif new_token_count >= expand_token_count and new_token_count > token_limit:   \n",
    "            partial_first_sentence = \" \".join([token.text for token in sent[(len(sent) - (token_limit - chunk_token_count)):]])\n",
    "            chunk = [partial_first_sentence] + chunk\n",
    "            chunk_token_count = token_limit\n",
    "            break\n",
    "        chunk_token_count = new_token_count\n",
    "        chunk = [sent.text] + chunk\n",
    "        chunk_starting_sentence_idx = current_sent_idx\n",
    "        \n",
    "        current_sent_idx -= 1\n",
    "    \n",
    "    # Join the sentences to form the chunk\n",
    "    chunk_text = \" \".join(chunk)\n",
    "       \n",
    "    return {\n",
    "        \"chunk\": chunk_text,\n",
    "        \"chunk_token_count\": chunk_token_count,\n",
    "        \"chunk_starting_sentence_idx\": chunk_starting_sentence_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebb5cab-935c-432e-8eea-500def7b0633",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(df_generated_data_middle.shape[0]):\n",
    "    if i % 50 == 0:\n",
    "        print(i)\n",
    "        print(\"Process time: \", round(time.time()-start_time, 2), \" seconds.\")\n",
    "        start_time = time.time()\n",
    "\n",
    "\n",
    "    chunk_context = df_generated_data_middle.at[i, 'context']\n",
    "    chunk_token_count = df_generated_data_middle.at[i, 'chunk_token_count']\n",
    "    doc_text = df_generated_data_middle.at[i, 'filing_text']\n",
    "    chunk_starting_sentence_idx = df_generated_data_middle.at[i, 'chunk_starting_sentence_idx'] \n",
    "\n",
    "    temp_target_token_count = chunk_token_count + (expand_token_count - chunk_token_count) // 2\n",
    "    temp_token_limit = temp_target_token_count + token_limit - expand_token_count\n",
    "    temp_chunk_result = expand_chunk_middle_1(nlp, doc_text, chunk_starting_sentence_idx, temp_target_token_count, temp_token_limit)\n",
    "\n",
    "    chunk_context = temp_chunk_result['chunk']\n",
    "    chunk_token_count = temp_chunk_result['chunk_token_count']\n",
    "    chunk_starting_sentence_idx = temp_chunk_result['chunk_starting_sentence_idx']\n",
    "    sentences = temp_chunk_result['sentences']\n",
    "\n",
    "    chunk_result = expand_chunk_middle_2(sentences, chunk_context, chunk_token_count, doc_text, chunk_starting_sentence_idx,\n",
    "                                expand_token_count, token_limit)    \n",
    "\n",
    "    df_generated_data_middle.at[i, 'context'] = chunk_result[\"chunk\"]\n",
    "    df_generated_data_middle.at[i, 'chunk_token_count'] = chunk_result[\"chunk_token_count\"]\n",
    "    df_generated_data_middle.at[i, 'chunk_starting_sentence_idx'] = chunk_result[\"chunk_starting_sentence_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8cdca-c9ff-4f29-8943-25371ece606e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df_generated_data_middle['chunk_token_count'], bins=30)\n",
    "plt.xlabel('Token count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of chunk length (token count)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f8b93-5db7-47c6-8547-efefb20498c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk.to_csv('Phantom_10k_10000tokens_middle_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4188af5b-fb12-40fd-98d8-990bc6c66287",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_middle = pd.DataFrame(columns = ['query', 'context', 'answer', 'ground_truth_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d4498-fa1c-4cf9-8d2e-4ad0ffd6f5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_generated_data_middle.shape[0]):\n",
    "    query = df_generated_data_middle.at[i, 'query']\n",
    "    context = df_generated_data_middle.at[i, 'context']\n",
    "    row_data1 = [query, context, df_generated_data_middle.at[i, 'gold_answer'], 'not hallucination']\n",
    "    row_data2 = [query, context, df_generated_data_middle.at[i, 'hallucination_answer'], 'hallucination']\n",
    "    df_final_middle.loc[len(df_final_middle)] = row_data1\n",
    "    df_final_middle.loc[len(df_final_middle)] = row_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f88fd2-4f40-48e3-9802-2533128b7363",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final_middle.to_csv('Phantom_10k_10000tokens_middle.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
