{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef8c7485-b39d-413f-80f0-db54bc37aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List\n",
    "\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import torch\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523a3617-7261-4f37-a4ee-9b465fc3a88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama33Model():\n",
    "    def __init__(self, model_path=\"distilgpt2\", batch_size=2): \n",
    "        #model_path=\"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "        self.model_path = model_path\n",
    "        self.batch_size = batch_size\n",
    "        self.pipeline = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=self.model_path,\n",
    "            model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "\n",
    "        terminators = [\n",
    "            self.pipeline.tokenizer.eos_token_id,\n",
    "            self.pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        ]\n",
    "\n",
    "        self.pipeline.tokenizer.padding_side = \"left\"\n",
    "\n",
    "    def generate_prompts(self, prompts: List[str]):\n",
    "        for prompt in prompts:\n",
    "            message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "            yield self.pipeline.tokenizer.apply_chat_template(\n",
    "                message,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "\n",
    "    def generate_responses(self, prompts: List[str], max_new_tokens: int = 5000, temperature: float = 1,\n",
    "                          top_p: float = 0.9, return_answer_only: bool = True):\n",
    "\n",
    "        output = self.pipeline(\n",
    "            self.generate_prompts(prompts),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            pad_token_id=self.pipeline.tokenizer.eos_token_id,\n",
    "            batch_size=self.batch_size\n",
    "        )\n",
    "\n",
    "        if return_answer_only:\n",
    "            return [outputs[0]['generated_text'].split('<|start_header_id|>assistant<|end_header_id|>')[1]\n",
    "                    for outputs in output]\n",
    "        else:\n",
    "            return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7317bf34-a18e-414b-ba37-866e20808553",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Llama33Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad484a4f-6aa5-4691-bba8-342b527ef9d6",
   "metadata": {},
   "source": [
    "# Create chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37307bd0-2834-4076-b088-e42c9343d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500 \n",
    "n_tokens_per_chunk = 500\n",
    "n_token_upper_limit_per_chunk = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd312f73-d5f4-4ab4-91fb-b0b932f7da39",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load file containing 10k documents info.\n",
    "Put the file here that contains the source documents (filings raw text).\n",
    "The file should be a pandas dataframe with a column named 'filing_text' that contains the text of the filings.\n",
    "\"\"\"\n",
    "df_data = pd.read_pickle(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4191d0-1a02-4878-b127-e68b6b7be8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk = df_data.sample(n=n_samples).copy()\n",
    "df_data_chunk = df_data_chunk.reset_index(drop=True)\n",
    "df_data_chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4776c1-fb30-41db-b7c5-3c6b4fe19c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_random_chunk(nlp, doc_text, token_count=500, token_limit=600):\n",
    "    doc = nlp(doc_text)\n",
    "\n",
    "    tokens = [token.text for token in doc]\n",
    "    total_tokens = len(tokens)\n",
    "    \n",
    "    # Extract sentences\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    # Randomly select a starting sentence\n",
    "    start_idx = random.randint(0, len(sentences) - 1)\n",
    "    \n",
    "    # Collect sentences until the token count is reached\n",
    "    chunk = []\n",
    "    chunk_token_count = 0\n",
    "\n",
    "    for sent in sentences[start_idx:]:\n",
    "        chunk.append(sent.text)\n",
    "        new_token_count = chunk_token_count + len(sent)\n",
    "        if new_token_count >= token_count and new_token_count <= token_limit:\n",
    "            chunk_token_count = new_token_count\n",
    "            break\n",
    "        elif new_token_count >= token_count and new_token_count > token_limit:   \n",
    "            partial_last_sentence = \" \".join([token.text for token in sent[:(token_limit - chunk_token_count)]])\n",
    "            chunk[-1] = partial_last_sentence\n",
    "            chunk_token_count = token_limit\n",
    "            break\n",
    "        chunk_token_count = new_token_count \n",
    "\n",
    "    if chunk_token_count < token_count:\n",
    "        for sentence_idx in range(start_idx, -1, -1):\n",
    "            sent = sentences[sentence_idx]\n",
    "            new_token_count = chunk_token_count + len(sent)\n",
    "            if new_token_count >= token_count and new_token_count <= token_limit:\n",
    "                chunk_token_count = new_token_count\n",
    "                chunk = [sent.text] + chunk\n",
    "                chunk_starting_sentence_idx = sentence_idx\n",
    "                break\n",
    "            elif new_token_count >= token_count and new_token_count > token_limit:   \n",
    "                break\n",
    "            chunk_token_count = new_token_count\n",
    "            chunk = [sent.text] + chunk\n",
    "            chunk_starting_sentence_idx = sentence_idx    \n",
    "    \n",
    "    \n",
    "    # Join the sentences to form the chunk\n",
    "    chunk_text = \" \".join(chunk)\n",
    "\n",
    "    return {\n",
    "        \"chunk\": chunk_text,\n",
    "        \"start_sentence_idx\": start_idx,\n",
    "        \"chunk_token_count\": chunk_token_count,\n",
    "        \"total_tokens\": total_tokens,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f055b8f-3404-45d1-ab15-649279e77684",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_data_chunk.shape[0]):\n",
    "    if i % 20 == 0:\n",
    "        print(i)\n",
    "    doc_text = df_data_chunk.at[i, 'filing_text']\n",
    "    if len(doc_text) >= 1000000:\n",
    "        doc_text = doc_text[-999999:]\n",
    "    df_data_chunk.at[i, 'filing_text'] = doc_text\n",
    "        \n",
    "    chunk_result = get_random_chunk(nlp, doc_text, token_count=n_tokens_per_chunk, token_limit=n_token_upper_limit_per_chunk)\n",
    "    df_data_chunk.at[i, 'context'] = chunk_result[\"chunk\"]\n",
    "    df_data_chunk.at[i, 'chunk_token_count'] = chunk_result[\"chunk_token_count\"]\n",
    "    df_data_chunk.at[i, 'document_token_count'] = chunk_result[\"total_tokens\"]\n",
    "    df_data_chunk.at[i, 'chunk_starting_sentence_idx'] = chunk_result[\"start_sentence_idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e1b62-389f-4a46-bfe1-76d8b4981c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk = df_data_chunk[df_data_chunk['chunk_token_count'] >= n_tokens_per_chunk].copy()\n",
    "df_data_chunk = df_data_chunk.reset_index(drop=True)\n",
    "df_data_chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b124367e-bc03-4322-b493-f5026e10c252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk['chunk_token_count'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59857b85-9510-4b0f-8b4f-943192c8477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_chunk.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0245efb-45f6-4034-b2b8-30f1a36ec57b",
   "metadata": {},
   "source": [
    "# Generate queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0163d98c-bf3b-41c1-9c60-627f101efb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data = df_data_chunk.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53dae51-9636-4f99-962d-907212b34c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_generated_data.shape[0]):\n",
    "    if i % 20 == 0:\n",
    "        print(i)\n",
    "    chunk =df_generated_data.at[i, 'context']\n",
    "\n",
    "    PROMPT_QUERY_GENERATION = f\"\"\"\n",
    "    You are a financial analyst. You are asked to write 1 question that can be answered by the information in the provided document chunk which is from 10-K filings. \n",
    "    ***[START OF DOCUMENT CHUNK]\n",
    "    {chunk}\n",
    "    ***[END OF DOCUMENT CHUNK]\n",
    "    \n",
    "    Guidelines:\n",
    "        1. You are a financial analyst. Imagine you're given a tool that you can ask questions about a SEC filing so that it saves your time reading the document. Come up with a question that you want to ask. It needs to be financial meaningful.\n",
    "        2. Try to avoid asking questions about small unmeaningful details in the SEC filings. Come up with realistic questions that a financial analyst might care about. \n",
    "        3. Make sure the question you generated can be answered with the information in the document chunk and does not require any other knowledge or information.\n",
    "    \n",
    "    You must return ONLY the question. Do not generate anything else other than the question.\n",
    "    \"\"\"\n",
    "    df_generated_data.at[i, 'query'] = model3.generate_response(PROMPT_QUERY_GENERATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d9439-ed65-4db2-b1b5-38c1cc55f757",
   "metadata": {},
   "source": [
    "## Generate gold answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b62362-dd9d-49c0-a299-94e54ca56c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_generated_data.shape[0]):\n",
    "    if i % 20 == 0:\n",
    "        print(i)    \n",
    "    query = df_generated_data.at[i, 'query']\n",
    "    chunk = df_generated_data.at[i, 'context']\n",
    "    \n",
    "    PROMPT_ANSWER_GENERATION = f\"\"\"\n",
    "    Answer the following question based on the information in the given document chunk.\n",
    "    [QUESTION]\n",
    "    {query}\n",
    "    [DOCUMENT CHUNK]\n",
    "    {chunk}\n",
    "\n",
    "    Provide answer to the QUESTION only using information from the DOCUMENT CHUNK provided. Make sure your answer is consistent with the information in the document chunk.\n",
    "    You answer should only include information that is supported by the document chunk.\n",
    "    If the document chunk does not contain enough information for you to answer the question, output \"Information is not available in the document.\"\n",
    "    You must output ONLY the answer. Do not generate anything else other than the answer.\n",
    "    \"\"\"\n",
    "\n",
    "    df_generated_data.at[i, 'gold_answer'] = model3.generate_response(PROMPT_ANSWER_GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe365408-9fe1-4ee4-a588-5c55adda7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cad34af-ce98-43e6-8968-cfec2f548cd3",
   "metadata": {},
   "source": [
    "### Generate hallucination answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220df863-ccfb-454a-94fa-94d6cb2b0ba1",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Replace [EXAMPLE 1] through [EXAMPLE 3] with custom few-shot examples of hallucinations in PROMPT_HALLUCINATION_GENERATION. Use the following guidelines in creating your examples:\n",
    "\n",
    "• Each example should present a clear question paired with a document excerpt and demonstrates how even small alterations in phrasing can lead to a hallucination.\n",
    "• The examples should showcase the importance of closely matching the document's context—e.g., misinterpreting price trends, omitting critical qualifiers, etc.\n",
    "• Examples can reveal how inserting extra assumptions (e.g., implying a strategic shift) can distort the intended meaning of the original content.\n",
    "• Examples can serve as instructive demonstrations for how automated summarization or extraction may deviate subtly from primary data, risking misinterpretations.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "for i in range(df_generated_data.shape[0]):\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(i)    \n",
    "        \n",
    "    query = df_generated_data.at[i, 'query']\n",
    "    chunk = df_generated_data.at[i, 'context']\n",
    "    answer = df_generated_data.at[i, 'gold_answer']\n",
    "\n",
    "\n",
    "    PROMPT_HALLUCINATION_GENERATION = f\"\"\"\n",
    "    Given a question, a correct answer and a reference document chunk, write a HALLUCINATION ANSWER to the question.\n",
    "    \n",
    "    [QUESTION]\n",
    "    {query}\n",
    "    [A CORRECT ANSWER]\n",
    "    {answer}\n",
    "    [DOCUMENT CHUNK]\n",
    "    ***\n",
    "    {chunk}\n",
    "    ***\n",
    "    \n",
    "    [INSTUCTION]\n",
    "    - You're given a CORRECT ANSWER to the QUESTION. The CORRECT ANSWER provided is consistent with the information in the DOCUMENT CHUNK.\n",
    "    - This HALLUCINATION ANSWER you need to write is mostly correct, but contains information that is not fully supported by the DOCUMENT CHUNK. The unsupported content in the HALLUCINATION ANSWER is minor and subtle.\n",
    "    - Be creative with writing the HALLUCINATION ANSWER. You know the domain terminology and jargon very well. Make it realistic sounding and hard to catch even for a domain expert. \n",
    "    - For example, write an answer that is mostly correct, but contains a small detail that does not match the context in the document chunk, or one part of the answer talks about something that does not exist in the document chunk, or one part of the answer is missing some details that causes the answer to be misleading.\n",
    "\n",
    "    [EXAMPLES]\n",
    "    Here are some examples of HALLUCINATION ANSWERs that are hard to catch.\n",
    "\n",
    "    [EXAMPLE 1]\n",
    "    \n",
    "    [EXAMPLE 2]\n",
    "    \n",
    "    [EXAMPLE 3] \n",
    "    \n",
    "    \n",
    "    [END OF EXAMPLES]\n",
    "    \n",
    "    You must output ONLY the HALLUCINATION ANSWER. Don't write anything other than the HALLUCINATION ANSWER itself.\n",
    "    \"\"\"\n",
    "    df_generated_data.at[i, 'hallucination_answer'] = model3.generate_response(PROMPT_HALLUCINATION_GENERATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d00023-efff-4236-948a-c0ab16e17d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7f31a6-f599-4d6e-9483-32c9ec3759fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated_data.to_csv('df_10k_llama3_3_v1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7d4fb8-24f7-464d-839b-16f871ca847b",
   "metadata": {},
   "source": [
    "## Reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63680d0-5a79-4ce7-ab51-2e11c2d6bc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.DataFrame(columns = ['query', 'context', 'answer', 'ground_truth_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8260445-0175-4ebd-a09d-f0b2070d6c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_generated_data.shape[0]):\n",
    "    query = df_generated_data.at[i, 'query']\n",
    "    context = df_generated_data.at[i, 'context']\n",
    "    row_data1 = [query, context, df_generated_data.at[i, 'gold_answer'], 'not hallucination']\n",
    "    row_data2 = [query, context, df_generated_data.at[i, 'hallucination_answer'], 'hallucination']\n",
    "    df_final.loc[len(df_final)] = row_data1\n",
    "    df_final.loc[len(df_final)] = row_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5831661e-a337-4f6a-b851-e720ffbe63e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('Phantom_10k_seed.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
